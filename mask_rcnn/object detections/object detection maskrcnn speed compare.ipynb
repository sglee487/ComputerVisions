{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import six.moves.urllib as urllib\n",
    "import sys\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "\n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import pytesseract\n",
    "import tensorflow as tf\n",
    "import re\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_detection.utils import ops as utils_ops\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as vis_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.gfile = tf.io.gfile\n",
    "\n",
    "utils_ops.tf = tf.compat.v1\n",
    "\n",
    "vis_util.tf = tf.compat.v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ssd_mobilenet_v1_coco_2018_01_28 모델 사용시  \n",
    "1. frozen_inference_graph 로 불러와 session 만들어서 직접 후 처리 (tf 1)\n",
    "2. saved_model 불러와 직접 후 처리\n",
    "3. tensorflow model research 에서 제공된 메소드 이용 처리  \n",
    "(튜토리얼(제공메소드) 방식: https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. frozen_inference_graph 로 불러와 session 만들어서 직접 후 처리 (tf 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프 정의\n",
    "# https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md\n",
    "with tf.compat.v1.gfile.GFile(\"ssd_mobilenet_v1_coco_2018_01_28/frozen_inference_graph.pb\",\"rb\") as f:\n",
    "    graph = tf.compat.v1.GraphDef()\n",
    "    graph.ParseFromString(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라벨 맵 불러오기 방법 2\n",
    "# https://developers.google.com/protocol-buffers/docs/downloads 에서 다운받아 proto 파일들을 컴파일 해야 된다.\n",
    "# https://github.com/tensorflow/models/blob/master/research/object_detection/data/mscoco_complete_label_map.pbtxt\n",
    "# https://github.com/tensorflow/models/blob/master/research/object_detection/utils/label_map_util.py\n",
    "from object_detection.utils.label_map_util import get_label_map_dict\n",
    "\n",
    "tf.gfile = tf.io.gfile\n",
    "get_label_map_dict.tf = tf.compat.v1\n",
    "\n",
    "label_map = get_label_map_dict(\"ssd_mobilenet_v1_coco_2018_01_28/mscoco_complete_label_map.pbtxt\", True)\n",
    "label = dict()\n",
    "for item in label_map.items():\n",
    "    label[item[1]] = item[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read time from 0 ~ to 20 : 4.371s\n",
      "read time from 20 ~ to 40 : 1.661s\n",
      "read time from 40 ~ to 60 : 1.656s\n",
      "read time from 60 ~ to 80 : 1.657s\n",
      "read time from 80 ~ to 100 : 1.654s\n",
      "read time from 100 ~ to 120 : 1.649s\n"
     ]
    }
   ],
   "source": [
    "# 세션 생성\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    tf.compat.v1.import_graph_def(graph, name='')\n",
    "    \n",
    "    # 추론 이미지 생성\n",
    "    capture = cv2.VideoCapture(\"files/bird.mp4\") # https://www.youtube.com/watch?v=XdlIbNrki5o\n",
    "\n",
    "    start = time.time()\n",
    "    while True:\n",
    "        ret, frame = capture.read()\n",
    "        \n",
    "        if (capture.get(cv2.CAP_PROP_POS_FRAMES) == capture.get(cv2.CAP_PROP_FRAME_COUNT)):\n",
    "            break\n",
    "            \n",
    "        input_img = cv2.resize(frame, (300,300))\n",
    "        input_img = cv2.cvtColor(input_img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # 그래프 연산 수행\n",
    "        num_detections, scores, classes, boxes = sess.run(\n",
    "            fetches=[sess.graph.get_tensor_by_name(\"num_detections:0\"),\n",
    "                   sess.graph.get_tensor_by_name(\"detection_scores:0\"),\n",
    "                   sess.graph.get_tensor_by_name(\"detection_classes:0\"),\n",
    "                   sess.graph.get_tensor_by_name(\"detection_boxes:0\")],\n",
    "        feed_dict={\"image_tensor:0\": input_img.reshape(1, input_img.shape[0], input_img.shape[1],\n",
    "                                                       input_img.shape[2])}\n",
    "        )\n",
    "        \n",
    "        # 추론 결과 표시\n",
    "        rows, cols, _ = frame.shape\n",
    "        for i in range(int(num_detections[0])):\n",
    "            class_id = int(classes[0][i])\n",
    "            score = scores[0][i]\n",
    "            box = boxes[0][i]\n",
    "            \n",
    "            if score > 0.7:\n",
    "                x1 = int(box[1] * cols)\n",
    "                y1 = int(box[0] * rows)\n",
    "                x2 = int(box[3] * cols)\n",
    "                y2 = int(box[2] * rows)\n",
    "                \n",
    "\n",
    "                cv2.putText(frame, label[class_id] + \":\" + str(score), (x1, y1 - 5), cv2.FONT_HERSHEY_COMPLEX,\n",
    "                            1.5, (0, 0, 255), 1)\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 5)\n",
    "\n",
    "\n",
    "        cv2.imshow(\"Object Detections\", frame)\n",
    "        if capture.get(cv2.CAP_PROP_POS_FRAMES) % 20 == 0:\n",
    "            print(\"read time from {:d} ~ to {:d} : {:.3f}s\".format(\n",
    "                int(capture.get(cv2.CAP_PROP_POS_FRAMES)-20),int(capture.get(cv2.CAP_PROP_POS_FRAMES)),time.time()-start))\n",
    "            start = time.time()\n",
    "        if cv2.waitKey(33) == ord('q'): \n",
    "            cv2.destroyAllWindows()\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. saved_model 불러와 직접 후 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_exist_model(model_name):\n",
    "    model_dir = model_name\n",
    "    model_dir = pathlib.Path(model_dir)/\"saved_model\"\n",
    "\n",
    "    model = tf.saved_model.load(str(model_dir))\n",
    "    model = model.signatures['serving_default']\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "model_name = \"ssd_mobilenet_v1_coco_2018_01_28\"\n",
    "model = load_exist_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/tensorflow/models/blob/master/research/object_detection/data/mscoco_label_map.pbtxt\n",
    "# 라벨 맵 불러오기 방법 1\n",
    "with open(\"ssd_mobilenet_v1_coco_2018_01_28/mscoco_label_map.pbtxt\",\"rt\") as f:\n",
    "    pb_classes = f.read().rstrip('\\n').split('\\n')\n",
    "    \n",
    "    classes_label = dict()\n",
    "    for i in range(0, len(pb_classes), 5):\n",
    "        pb_classId = int(re.findall(r\"\\d+\", pb_classes[i+2])[0])\n",
    "        pattern = r\"display_name: \\\"(.*?)\\\"\"\n",
    "        pb_text = re.search(pattern, pb_classes[i+3])\n",
    "        classes_label[pb_classId] = pb_text.group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # List of the strings that is used to add correct label for each box.\n",
    "# PATH_TO_LABELS = 'object_detection/data/mscoco_label_map.pbtxt'\n",
    "# category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'detection_scores': tf.float32,\n",
       " 'detection_classes': tf.float32,\n",
       " 'num_detections': tf.float32,\n",
       " 'detection_boxes': tf.float32}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.output_dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'detection_scores': TensorShape([None, 100]),\n",
       " 'detection_classes': TensorShape([None, 100]),\n",
       " 'num_detections': TensorShape([None]),\n",
       " 'detection_boxes': TensorShape([None, 100, 4])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.output_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read time from 0 ~ to 20 : 6.435s\n",
      "read time from 20 ~ to 40 : 1.742s\n",
      "read time from 40 ~ to 60 : 1.717s\n",
      "read time from 60 ~ to 80 : 1.725s\n",
      "read time from 80 ~ to 100 : 1.753s\n",
      "read time from 100 ~ to 120 : 1.701s\n",
      "read time from 120 ~ to 140 : 1.696s\n",
      "read time from 140 ~ to 160 : 1.656s\n",
      "read time from 160 ~ to 180 : 1.738s\n",
      "read time from 180 ~ to 200 : 1.746s\n",
      "read time from 200 ~ to 220 : 1.732s\n",
      "read time from 220 ~ to 240 : 1.728s\n",
      "read time from 240 ~ to 260 : 1.730s\n",
      "read time from 260 ~ to 280 : 1.730s\n",
      "read time from 280 ~ to 300 : 1.741s\n",
      "read time from 300 ~ to 320 : 1.830s\n",
      "read time from 320 ~ to 340 : 1.743s\n",
      "read time from 340 ~ to 360 : 2.342s\n",
      "read time from 360 ~ to 380 : 2.540s\n",
      "read time from 380 ~ to 400 : 2.532s\n",
      "read time from 400 ~ to 420 : 2.496s\n",
      "read time from 420 ~ to 440 : 2.661s\n",
      "read time from 440 ~ to 460 : 2.096s\n",
      "read time from 460 ~ to 480 : 2.565s\n",
      "read time from 480 ~ to 500 : 1.762s\n",
      "read time from 500 ~ to 520 : 1.747s\n",
      "read time from 520 ~ to 540 : 2.493s\n",
      "read time from 540 ~ to 560 : 2.569s\n",
      "read time from 560 ~ to 580 : 1.757s\n",
      "read time from 580 ~ to 600 : 1.723s\n",
      "read time from 600 ~ to 620 : 1.737s\n",
      "read time from 620 ~ to 640 : 1.721s\n",
      "read time from 640 ~ to 660 : 1.780s\n",
      "read time from 660 ~ to 680 : 2.161s\n",
      "read time from 680 ~ to 700 : 1.929s\n",
      "read time from 700 ~ to 720 : 1.702s\n",
      "read time from 720 ~ to 740 : 1.720s\n",
      "read time from 740 ~ to 760 : 1.779s\n",
      "read time from 760 ~ to 780 : 1.751s\n",
      "read time from 780 ~ to 800 : 1.727s\n",
      "read time from 800 ~ to 820 : 1.759s\n",
      "read time from 820 ~ to 840 : 1.712s\n"
     ]
    }
   ],
   "source": [
    "#비디오에서 적용\n",
    "\n",
    "capture = cv2.VideoCapture(\"files/bird.mp4\") # https://www.youtube.com/watch?v=XdlIbNrki5o\n",
    "\n",
    "start = time.time()\n",
    "while True:\n",
    "    \n",
    "    ret, frame = capture.read()\n",
    "    \n",
    "    if (capture.get(cv2.CAP_PROP_POS_FRAMES) == capture.get(cv2.CAP_PROP_FRAME_COUNT)):\n",
    "        break\n",
    "        \n",
    "    image = np.asarray(frame)\n",
    "    input_tensor = tf.convert_to_tensor(image)\n",
    "    input_tensor = input_tensor[tf.newaxis,...]\n",
    "\n",
    "    output_dict = model(input_tensor)\n",
    "\n",
    "    # 추론 결과 표시\n",
    "    rows, cols, _ = image.shape\n",
    "    for i in range(int(output_dict['num_detections'])):\n",
    "        class_id = int(output_dict['detection_classes'][0][i])\n",
    "        score = output_dict['detection_scores'][0][i].numpy()\n",
    "        box = output_dict['detection_boxes'][0][i]\n",
    "\n",
    "        if score > 0.7:\n",
    "            x1 = int(box[1] * cols)\n",
    "            y1 = int(box[0] * rows)\n",
    "            x2 = int(box[3] * cols)\n",
    "            y2 = int(box[2] * rows)\n",
    "\n",
    "\n",
    "            cv2.putText(image, classes_label[class_id] + \":\" + str(score), (x1, y1 - 5), cv2.FONT_HERSHEY_COMPLEX,\n",
    "                        1.5, (0, 0, 255), 1)\n",
    "            cv2.rectangle(image, (x1, y1), (x2, y2), (255, 0, 0), 5)\n",
    "\n",
    "    cv2.imshow(\"Object Detections\", image)\n",
    "    if capture.get(cv2.CAP_PROP_POS_FRAMES) % 20 == 0:\n",
    "            print(\"read time from {:d} ~ to {:d} : {:.3f}s\".format(\n",
    "                int(capture.get(cv2.CAP_PROP_POS_FRAMES)-20),int(capture.get(cv2.CAP_PROP_POS_FRAMES)),time.time()-start))\n",
    "            start = time.time()\n",
    "    if cv2.waitKey(33) == ord('q'): \n",
    "        cv2.destroyAllWindows()\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. tensorflow model research 에서 제공된 메소드 이용 처리 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_exist_model(model_name):\n",
    "    model_dir = model_name\n",
    "    model_dir = pathlib.Path(model_dir)/\"saved_model\"\n",
    "\n",
    "    model = tf.saved_model.load(str(model_dir))\n",
    "    model = model.signatures['serving_default']\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_for_single_image(model, image):\n",
    "    image = np.asarray(image)\n",
    "    # The input needs to be a tensor, convert it using `tf.convert_to_tensor`.\n",
    "    input_tensor = tf.convert_to_tensor(image)\n",
    "    # The model expects a batch of images, so add an axis with `tf.newaxis`.\n",
    "    input_tensor = input_tensor[tf.newaxis,...]\n",
    "\n",
    "    # Run inference\n",
    "    output_dict = model(input_tensor)\n",
    "\n",
    "    # All outputs are batches tensors.\n",
    "    # Convert to numpy arrays, and take index [0] to remove the batch dimension.\n",
    "    # We're only interested in the first num_detections.\n",
    "    num_detections = int(output_dict.pop('num_detections'))\n",
    "    output_dict = {key:value[0, :num_detections].numpy() \n",
    "                                 for key,value in output_dict.items()}\n",
    "    output_dict['num_detections'] = num_detections\n",
    "\n",
    "    # detection_classes should be ints.\n",
    "    output_dict['detection_classes'] = output_dict['detection_classes'].astype(np.int64)\n",
    "     \n",
    "    # Handle models with masks:\n",
    "    if 'detection_masks' in output_dict:\n",
    "        # Reframe the the bbox mask to the image size.\n",
    "        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "                            output_dict['detection_masks'], output_dict['detection_boxes'],\n",
    "                             image.shape[0], image.shape[1])            \n",
    "        detection_masks_reframed = tf.cast(detection_masks_reframed > 0.5,\n",
    "                                                                             tf.uint8)\n",
    "        output_dict['detection_masks_reframed'] = detection_masks_reframed.numpy()\n",
    "        \n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_inference_test2(model, image):\n",
    "    # the array based representation of the image will be used later in order to prepare the\n",
    "    # result image with boxes and labels on it.\n",
    "    image_np = np.array(image)\n",
    "    # Actual detection.\n",
    "    output_dict = run_inference_for_single_image(model, image_np)\n",
    "    # Visualization of the results of a detection.\n",
    "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "            image_np,\n",
    "            output_dict['detection_boxes'],\n",
    "            output_dict['detection_classes'],\n",
    "            output_dict['detection_scores'],\n",
    "            category_index,\n",
    "            instance_masks=output_dict.get('detection_masks_reframed', None),\n",
    "            use_normalized_coordinates=True,\n",
    "            min_score_thresh = .7,\n",
    "            line_thickness=8)\n",
    "\n",
    "    return image_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "model_name = \"ssd_mobilenet_v1_coco_2018_01_28\"\n",
    "model = load_exist_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of the strings that is used to add correct label for each box.\n",
    "PATH_TO_LABELS = 'object_detection/data/mscoco_label_map.pbtxt'\n",
    "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read time from 0 ~ to 20 : 4.973s\n",
      "read time from 20 ~ to 40 : 1.932s\n",
      "read time from 40 ~ to 60 : 1.931s\n",
      "read time from 60 ~ to 80 : 1.932s\n",
      "read time from 80 ~ to 100 : 1.928s\n",
      "read time from 100 ~ to 120 : 1.986s\n",
      "read time from 120 ~ to 140 : 1.985s\n",
      "read time from 140 ~ to 160 : 1.982s\n",
      "read time from 160 ~ to 180 : 1.959s\n",
      "read time from 180 ~ to 200 : 1.952s\n",
      "read time from 200 ~ to 220 : 1.990s\n",
      "read time from 220 ~ to 240 : 1.982s\n",
      "read time from 240 ~ to 260 : 1.974s\n",
      "read time from 260 ~ to 280 : 1.992s\n",
      "read time from 280 ~ to 300 : 1.995s\n",
      "read time from 300 ~ to 320 : 2.076s\n"
     ]
    }
   ],
   "source": [
    "# 비디오에서 적용    \n",
    "# 추론 이미지 생성\n",
    "capture = cv2.VideoCapture(\"files/bird.mp4\") # https://www.youtube.com/watch?v=XdlIbNrki5o\n",
    "start = time.time()\n",
    "while True:\n",
    "    ret, frame = capture.read()\n",
    "        \n",
    "    if (capture.get(cv2.CAP_PROP_POS_FRAMES) == capture.get(cv2.CAP_PROP_FRAME_COUNT)):\n",
    "        break\n",
    "        \n",
    "    frame = show_inference_test2(model, frame)\n",
    "\n",
    "    cv2.imshow(\"Object Detections\", frame)\n",
    "    \n",
    "    if capture.get(cv2.CAP_PROP_POS_FRAMES) % 20 == 0:\n",
    "            print(\"read time from {:d} ~ to {:d} : {:.3f}s\".format(\n",
    "                int(capture.get(cv2.CAP_PROP_POS_FRAMES)-20),int(capture.get(cv2.CAP_PROP_POS_FRAMES)),time.time()-start))\n",
    "            start = time.time()\n",
    "            \n",
    "    if cv2.waitKey(33) == ord('q'): \n",
    "        cv2.destroyAllWindows()\n",
    "        break\n",
    "    #     cv2.imshow(\"Object Detections\", img)\n",
    "    #     cv2.waitKey(0) \n",
    "    #     cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mask_rcnn_inception_v2_coco_2018_01_28 모델 사용시  \n",
    "1. frozen_inference_graph 로 불러와 session 만들어서 직접 후 처리 (tf 1)\n",
    "2. saved_model 불러와 직접 후 처리\n",
    "3. tensorflow model research 에서 제공된 메소드 이용 처리  \n",
    "(튜토리얼(제공메소드) 방식: https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. frozen_inference_graph 로 불러와 session 만들어서 직접 후 처리 (tf 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "반복문 seesion 안에서도 계속 seesion을 실행하기 때문에 너무 오래걸리는 것 같다. 이건 내가 하는 방법을 몰라서 그런듯.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.ImageColor as ImageColor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/tensorflow/models/blob/master/research/object_detection/utils/visualization_utils.py\n",
    "STANDARD_COLORS = [\n",
    "    'AliceBlue', 'Chartreuse', 'Aqua', 'Aquamarine', 'Azure', 'Beige', 'Bisque',\n",
    "    'BlanchedAlmond', 'BlueViolet', 'BurlyWood', 'CadetBlue', 'AntiqueWhite',\n",
    "    'Chocolate', 'Coral', 'CornflowerBlue', 'Cornsilk', 'Crimson', 'Cyan',\n",
    "    'DarkCyan', 'DarkGoldenRod', 'DarkGrey', 'DarkKhaki', 'DarkOrange',\n",
    "    'DarkOrchid', 'DarkSalmon', 'DarkSeaGreen', 'DarkTurquoise', 'DarkViolet',\n",
    "    'DeepPink', 'DeepSkyBlue', 'DodgerBlue', 'FireBrick', 'FloralWhite',\n",
    "    'ForestGreen', 'Fuchsia', 'Gainsboro', 'GhostWhite', 'Gold', 'GoldenRod',\n",
    "    'Salmon', 'Tan', 'HoneyDew', 'HotPink', 'IndianRed', 'Ivory', 'Khaki',\n",
    "    'Lavender', 'LavenderBlush', 'LawnGreen', 'LemonChiffon', 'LightBlue',\n",
    "    'LightCoral', 'LightCyan', 'LightGoldenRodYellow', 'LightGray', 'LightGrey',\n",
    "    'LightGreen', 'LightPink', 'LightSalmon', 'LightSeaGreen', 'LightSkyBlue',\n",
    "    'LightSlateGray', 'LightSlateGrey', 'LightSteelBlue', 'LightYellow', 'Lime',\n",
    "    'LimeGreen', 'Linen', 'Magenta', 'MediumAquaMarine', 'MediumOrchid',\n",
    "    'MediumPurple', 'MediumSeaGreen', 'MediumSlateBlue', 'MediumSpringGreen',\n",
    "    'MediumTurquoise', 'MediumVioletRed', 'MintCream', 'MistyRose', 'Moccasin',\n",
    "    'NavajoWhite', 'OldLace', 'Olive', 'OliveDrab', 'Orange', 'OrangeRed',\n",
    "    'Orchid', 'PaleGoldenRod', 'PaleGreen', 'PaleTurquoise', 'PaleVioletRed',\n",
    "    'PapayaWhip', 'PeachPuff', 'Peru', 'Pink', 'Plum', 'PowderBlue', 'Purple',\n",
    "    'Red', 'RosyBrown', 'RoyalBlue', 'SaddleBrown', 'Green', 'SandyBrown',\n",
    "    'SeaGreen', 'SeaShell', 'Sienna', 'Silver', 'SkyBlue', 'SlateBlue',\n",
    "    'SlateGray', 'SlateGrey', 'Snow', 'SpringGreen', 'SteelBlue', 'GreenYellow',\n",
    "    'Teal', 'Thistle', 'Tomato', 'Turquoise', 'Violet', 'Wheat', 'White',\n",
    "    'WhiteSmoke', 'Yellow', 'YellowGreen'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프 정의\n",
    "# https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md\n",
    "with tf.compat.v1.gfile.GFile(\"mask_rcnn_inception_v2_coco_2018_01_28/frozen_inference_graph.pb\",\"rb\") as f:\n",
    "    graph = tf.compat.v1.GraphDef()\n",
    "    graph.ParseFromString(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/tensorflow/models/blob/master/research/object_detection/data/mscoco_label_map.pbtxt\n",
    "# 라벨 맵 불러오기 방법 1\n",
    "with open(\"object_detection/data/mscoco_label_map.pbtxt\",\"rt\") as f:\n",
    "    pb_classes = f.read().rstrip('\\n').split('\\n')\n",
    "    \n",
    "    classes_label = dict()\n",
    "    for i in range(0, len(pb_classes), 5):\n",
    "        pb_classId = int(re.findall(r\"\\d+\", pb_classes[i+2])[0])\n",
    "        pattern = r\"display_name: \\\"(.*?)\\\"\"\n",
    "        pb_text = re.search(pattern, pb_classes[i+3])\n",
    "        classes_label[pb_classId] = pb_text.group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read time from 0 ~ to 20 : 15.158s\n",
      "read time from 20 ~ to 40 : 16.123s\n",
      "read time from 40 ~ to 60 : 19.095s\n",
      "read time from 60 ~ to 80 : 15.964s\n",
      "read time from 80 ~ to 100 : 14.373s\n",
      "read time from 100 ~ to 120 : 19.174s\n"
     ]
    }
   ],
   "source": [
    "# 세션 생성\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    tf.compat.v1.import_graph_def(graph, name='')\n",
    "    \n",
    "    # 추론 이미지 생성\n",
    "    capture = cv2.VideoCapture(\"files/bird.mp4\") # https://www.youtube.com/watch?v=XdlIbNrki5o\n",
    "\n",
    "    start = time.time()\n",
    "    while True:\n",
    "        ret, frame = capture.read()\n",
    "        \n",
    "        if (capture.get(cv2.CAP_PROP_POS_FRAMES) == capture.get(cv2.CAP_PROP_FRAME_COUNT)):\n",
    "            break\n",
    "            \n",
    "        input_img = cv2.resize(frame, (300,300))\n",
    "        input_img = cv2.cvtColor(input_img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # 그래프 연산 수행\n",
    "        num_detections, scores, classes, boxes, masks = sess.run(\n",
    "            fetches=[sess.graph.get_tensor_by_name(\"num_detections:0\"),\n",
    "                    sess.graph.get_tensor_by_name(\"detection_scores:0\"),\n",
    "                    sess.graph.get_tensor_by_name(\"detection_classes:0\"),\n",
    "                    sess.graph.get_tensor_by_name(\"detection_boxes:0\"),\n",
    "                    sess.graph.get_tensor_by_name(\"detection_masks:0\")],\n",
    "        feed_dict={\"image_tensor:0\": input_img.reshape(1, input_img.shape[0], input_img.shape[1],\n",
    "                                                        input_img.shape[2])}\n",
    "        )\n",
    "        \n",
    "        # 추론 결과 표시\n",
    "        rows, cols, _ = frame.shape\n",
    "        for i in range(int(num_detections[0])):\n",
    "            class_id = int(classes[0][i])\n",
    "            score = scores[0][i]\n",
    "            box = boxes[0][i]\n",
    "            mask = masks[0][i]\n",
    "            \n",
    "            if score > 0.7:\n",
    "                x1 = int(box[1] * cols)\n",
    "                y1 = int(box[0] * rows)\n",
    "                x2 = int(box[3] * cols)\n",
    "                y2 = int(box[2] * rows)\n",
    "                \n",
    "\n",
    "                cv2.putText(frame, classes_label[class_id] + \":\" + str(score), (x1, y1 - 5), cv2.FONT_HERSHEY_COMPLEX,\n",
    "                            1.5, (0, 0, 255), 1)\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 5)\n",
    "                \n",
    "#                 mask = tf.expand_dims(mask,axis=-1)\n",
    "                mask = tf.compat.v1.expand_dims(mask,axis=-1)\n",
    "#                 mask = sess.run(mask)\n",
    "                mask = tf.compat.v1.image.resize(mask,(y2-y1,x2-x1))\n",
    "#                 mask = sess.run(mask)\n",
    "                mask_reframed = tf.compat.v1.cast(mask > 0.5, tf.uint8)\n",
    "#                 mask_reframed = sess.run(mask_reframed)\n",
    "                mask_reframed = tf.compat.v1.squeeze(mask_reframed)\n",
    "                mask_reframed = sess.run(mask_reframed)\n",
    "                # for display\n",
    "        #         mask_reframed_d = mask_reframed * 255\n",
    "        #         display(Image.fromarray(mask_reframed_d.numpy()))\n",
    "\n",
    "                full_mask = np.zeros((rows, cols), dtype=np.uint8)\n",
    "                full_mask[y1:y2,x1:x2] = mask_reframed\n",
    "                # for display\n",
    "        #         full_mask_d = full_mask * 255\n",
    "        #         display(Image.fromarray(full_mask_d))\n",
    "\n",
    "                image = np.asarray(frame)\n",
    "                pil_image = Image.fromarray(image)\n",
    "                color = STANDARD_COLORS[class_id%len(STANDARD_COLORS)]\n",
    "\n",
    "                alpha = 0.4\n",
    "                rgb = ImageColor.getrgb(color)\n",
    "\n",
    "                solid_color = np.expand_dims(np.ones_like(full_mask), axis=2) * np.reshape(list(rgb), [1, 1, 3])\n",
    "                pil_solid_color = Image.fromarray(np.uint8(solid_color)).convert('RGBA')\n",
    "                pil_mask = Image.fromarray(np.uint8(255.0*alpha*full_mask)).convert('L')\n",
    "                pil_image = Image.composite(pil_solid_color, pil_image, pil_mask)\n",
    "                np.copyto(image, np.array(pil_image.convert('RGB')))\n",
    "\n",
    "\n",
    "        cv2.imshow(\"Object Detections\", frame)\n",
    "        if capture.get(cv2.CAP_PROP_POS_FRAMES) % 20 == 0:\n",
    "            print(\"read time from {:d} ~ to {:d} : {:.3f}s\".format(\n",
    "                int(capture.get(cv2.CAP_PROP_POS_FRAMES)-20),int(capture.get(cv2.CAP_PROP_POS_FRAMES)),time.time()-start))\n",
    "            start = time.time()\n",
    "        if cv2.waitKey(33) == ord('q'): \n",
    "            cv2.destroyAllWindows()\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. saved_model 불러와 직접 후 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.ImageColor as ImageColor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_exist_model(model_name):\n",
    "    model_dir = model_name\n",
    "    model_dir = pathlib.Path(model_dir)/\"saved_model\"\n",
    "\n",
    "    model = tf.saved_model.load(str(model_dir))\n",
    "    model = model.signatures['serving_default']\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/tensorflow/models/blob/master/research/object_detection/utils/visualization_utils.py\n",
    "STANDARD_COLORS = [\n",
    "    'AliceBlue', 'Chartreuse', 'Aqua', 'Aquamarine', 'Azure', 'Beige', 'Bisque',\n",
    "    'BlanchedAlmond', 'BlueViolet', 'BurlyWood', 'CadetBlue', 'AntiqueWhite',\n",
    "    'Chocolate', 'Coral', 'CornflowerBlue', 'Cornsilk', 'Crimson', 'Cyan',\n",
    "    'DarkCyan', 'DarkGoldenRod', 'DarkGrey', 'DarkKhaki', 'DarkOrange',\n",
    "    'DarkOrchid', 'DarkSalmon', 'DarkSeaGreen', 'DarkTurquoise', 'DarkViolet',\n",
    "    'DeepPink', 'DeepSkyBlue', 'DodgerBlue', 'FireBrick', 'FloralWhite',\n",
    "    'ForestGreen', 'Fuchsia', 'Gainsboro', 'GhostWhite', 'Gold', 'GoldenRod',\n",
    "    'Salmon', 'Tan', 'HoneyDew', 'HotPink', 'IndianRed', 'Ivory', 'Khaki',\n",
    "    'Lavender', 'LavenderBlush', 'LawnGreen', 'LemonChiffon', 'LightBlue',\n",
    "    'LightCoral', 'LightCyan', 'LightGoldenRodYellow', 'LightGray', 'LightGrey',\n",
    "    'LightGreen', 'LightPink', 'LightSalmon', 'LightSeaGreen', 'LightSkyBlue',\n",
    "    'LightSlateGray', 'LightSlateGrey', 'LightSteelBlue', 'LightYellow', 'Lime',\n",
    "    'LimeGreen', 'Linen', 'Magenta', 'MediumAquaMarine', 'MediumOrchid',\n",
    "    'MediumPurple', 'MediumSeaGreen', 'MediumSlateBlue', 'MediumSpringGreen',\n",
    "    'MediumTurquoise', 'MediumVioletRed', 'MintCream', 'MistyRose', 'Moccasin',\n",
    "    'NavajoWhite', 'OldLace', 'Olive', 'OliveDrab', 'Orange', 'OrangeRed',\n",
    "    'Orchid', 'PaleGoldenRod', 'PaleGreen', 'PaleTurquoise', 'PaleVioletRed',\n",
    "    'PapayaWhip', 'PeachPuff', 'Peru', 'Pink', 'Plum', 'PowderBlue', 'Purple',\n",
    "    'Red', 'RosyBrown', 'RoyalBlue', 'SaddleBrown', 'Green', 'SandyBrown',\n",
    "    'SeaGreen', 'SeaShell', 'Sienna', 'Silver', 'SkyBlue', 'SlateBlue',\n",
    "    'SlateGray', 'SlateGrey', 'Snow', 'SpringGreen', 'SteelBlue', 'GreenYellow',\n",
    "    'Teal', 'Thistle', 'Tomato', 'Turquoise', 'Violet', 'Wheat', 'White',\n",
    "    'WhiteSmoke', 'Yellow', 'YellowGreen'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/tensorflow/models/blob/master/research/object_detection/data/mscoco_label_map.pbtxt\n",
    "# 라벨 맵 불러오기 방법 1\n",
    "with open(\"object_detection/data/mscoco_label_map.pbtxt\",\"rt\") as f:\n",
    "    pb_classes = f.read().rstrip('\\n').split('\\n')\n",
    "    \n",
    "    classes_label = dict()\n",
    "    for i in range(0, len(pb_classes), 5):\n",
    "        pb_classId = int(re.findall(r\"\\d+\", pb_classes[i+2])[0])\n",
    "        pattern = r\"display_name: \\\"(.*?)\\\"\"\n",
    "        pb_text = re.search(pattern, pb_classes[i+3])\n",
    "        classes_label[pb_classId] = pb_text.group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "model_name = \"mask_rcnn_inception_v2_coco_2018_01_28\"\n",
    "model = load_exist_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # List of the strings that is used to add correct label for each box.\n",
    "# PATH_TO_LABELS = 'object_detection/data/mscoco_label_map.pbtxt'\n",
    "# category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'detection_scores': tf.float32,\n",
       " 'detection_classes': tf.float32,\n",
       " 'detection_masks': tf.float32,\n",
       " 'num_detections': tf.float32,\n",
       " 'detection_boxes': tf.float32}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.output_dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'detection_scores': TensorShape([None, 100]),\n",
       " 'detection_classes': TensorShape([None, 100]),\n",
       " 'detection_masks': TensorShape([None, None, None, None]),\n",
       " 'num_detections': TensorShape([None]),\n",
       " 'detection_boxes': TensorShape([None, 100, 4])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.output_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read time from 0 ~ to 20 : 15.723s\n",
      "read time from 20 ~ to 40 : 5.793s\n",
      "read time from 40 ~ to 60 : 5.274s\n",
      "read time from 60 ~ to 80 : 5.244s\n",
      "read time from 80 ~ to 100 : 5.664s\n",
      "read time from 100 ~ to 120 : 5.354s\n",
      "read time from 120 ~ to 140 : 5.470s\n",
      "read time from 140 ~ to 160 : 5.572s\n",
      "read time from 160 ~ to 180 : 5.824s\n",
      "read time from 180 ~ to 200 : 5.663s\n",
      "read time from 200 ~ to 220 : 5.621s\n",
      "read time from 220 ~ to 240 : 5.492s\n",
      "read time from 240 ~ to 260 : 5.390s\n",
      "read time from 260 ~ to 280 : 5.528s\n",
      "read time from 280 ~ to 300 : 5.171s\n",
      "read time from 300 ~ to 320 : 5.563s\n"
     ]
    }
   ],
   "source": [
    "#비디오에서 적용\n",
    "\n",
    "capture = cv2.VideoCapture(\"files/bird.mp4\") # https://www.youtube.com/watch?v=XdlIbNrki5o\n",
    "\n",
    "start = time.time()\n",
    "while True:\n",
    "    \n",
    "    ret, frame = capture.read()\n",
    "    \n",
    "    if (capture.get(cv2.CAP_PROP_POS_FRAMES) == capture.get(cv2.CAP_PROP_FRAME_COUNT)):\n",
    "        break\n",
    "        \n",
    "    image = np.asarray(frame)\n",
    "    input_tensor = tf.convert_to_tensor(image)\n",
    "    input_tensor = input_tensor[tf.newaxis,...]\n",
    "\n",
    "    output_dict = model(input_tensor)\n",
    "\n",
    "    # 추론 결과 표시\n",
    "    rows, cols, _ = image.shape\n",
    "    for i in range(int(output_dict['num_detections'])):\n",
    "        class_id = int(output_dict['detection_classes'][0][i])\n",
    "        score = output_dict['detection_scores'][0][i].numpy()\n",
    "        box = output_dict['detection_boxes'][0][i]\n",
    "        mask = output_dict['detection_masks'][0][i]\n",
    "\n",
    "        if score > 0.7:\n",
    "            x1 = int(box[1] * cols)\n",
    "            y1 = int(box[0] * rows)\n",
    "            x2 = int(box[3] * cols)\n",
    "            y2 = int(box[2] * rows)\n",
    "\n",
    "\n",
    "            cv2.putText(image, classes_label[class_id] + \":\" + str(score), (x1, y1 - 5), cv2.FONT_HERSHEY_COMPLEX,\n",
    "                        1.5, (0, 0, 255), 1)\n",
    "            cv2.rectangle(image, (x1, y1), (x2, y2), (255, 0, 0), 5)\n",
    "            \n",
    "            mask = tf.expand_dims(mask,axis=-1)        \n",
    "            mask = tf.image.resize(mask,(y2-y1,x2-x1))\n",
    "            mask_reframed = tf.cast(mask > 0.5, tf.uint8)\n",
    "            mask_reframed = tf.squeeze(mask_reframed)\n",
    "            # for display\n",
    "    #         mask_reframed_d = mask_reframed * 255\n",
    "    #         display(Image.fromarray(mask_reframed_d.numpy()))\n",
    "\n",
    "            full_mask = np.zeros((rows, cols), dtype=np.uint8)\n",
    "            full_mask[y1:y2,x1:x2] = mask_reframed\n",
    "            # for display\n",
    "    #         full_mask_d = full_mask * 255\n",
    "    #         display(Image.fromarray(full_mask_d))\n",
    "\n",
    "            pil_image = Image.fromarray(image)\n",
    "            color = STANDARD_COLORS[class_id%len(STANDARD_COLORS)]\n",
    "\n",
    "            alpha = 0.4\n",
    "            rgb = ImageColor.getrgb(color)\n",
    "\n",
    "            solid_color = np.expand_dims(np.ones_like(full_mask), axis=2) * np.reshape(list(rgb), [1, 1, 3])\n",
    "            pil_solid_color = Image.fromarray(np.uint8(solid_color)).convert('RGBA')\n",
    "            pil_mask = Image.fromarray(np.uint8(255.0*alpha*full_mask)).convert('L')\n",
    "            pil_image = Image.composite(pil_solid_color, pil_image, pil_mask)\n",
    "            np.copyto(image, np.array(pil_image.convert('RGB')))\n",
    "            \n",
    "\n",
    "    cv2.imshow(\"Object Detections\", image)\n",
    "    if capture.get(cv2.CAP_PROP_POS_FRAMES) % 20 == 0:\n",
    "            print(\"read time from {:d} ~ to {:d} : {:.3f}s\".format(\n",
    "                int(capture.get(cv2.CAP_PROP_POS_FRAMES)-20),int(capture.get(cv2.CAP_PROP_POS_FRAMES)),time.time()-start))\n",
    "            start = time.time()\n",
    "    if cv2.waitKey(33) == ord('q'): \n",
    "        cv2.destroyAllWindows()\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. tensorflow model research 에서 제공된 메소드 이용 처리  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_for_single_image_test(output_dict, image):\n",
    "    image = np.asarray(image)\n",
    "    \n",
    "    # All outputs are batches tensors.\n",
    "    # Convert to numpy arrays, and take index [0] to remove the batch dimension.\n",
    "    # We're only interested in the first num_detections.\n",
    "    num_detections = int(output_dict.pop('num_detections'))\n",
    "    \n",
    "    output_dict = {key:value[0, :num_detections].numpy() \n",
    "                                 for key,value in output_dict.items()}\n",
    "    output_dict['num_detections'] = num_detections\n",
    "    \n",
    "    # detection_classes should be ints.\n",
    "    output_dict['detection_classes'] = output_dict['detection_classes'].astype(np.int64)\n",
    "    \n",
    "    # Handle models with masks:\n",
    "    if 'detection_masks' in output_dict:\n",
    "        # Reframe the the bbox mask to the image size.\n",
    "        \n",
    "        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "                            output_dict['detection_masks'], output_dict['detection_boxes'],\n",
    "                             image.shape[0], image.shape[1])            \n",
    "        detection_masks_reframed = tf.cast(detection_masks_reframed > 0.5,\n",
    "                                                                             tf.uint8)\n",
    "        output_dict['detection_masks_reframed'] = detection_masks_reframed.numpy()\n",
    "        \n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_inference_test(output_dict, image):\n",
    "    # the array based representation of the image will be used later in order to prepare the\n",
    "    # result image with boxes and labels on it.\n",
    "    image_np = np.array(image)\n",
    "    # Actual detection.\n",
    "    output_dict = run_inference_for_single_image_test(output_dict, image_np)\n",
    "    \n",
    "    # Visualization of the results of a detection.\n",
    "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "            image_np,\n",
    "            output_dict['detection_boxes'],\n",
    "            output_dict['detection_classes'],\n",
    "            output_dict['detection_scores'],\n",
    "            category_index,\n",
    "            instance_masks=output_dict.get('detection_masks_reframed', None),\n",
    "            use_normalized_coordinates=True,\n",
    "            min_score_thresh = .7,\n",
    "            line_thickness=8)\n",
    "    \n",
    "\n",
    "#     display(Image.fromarray(image_np))\n",
    "    \n",
    "    return image_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of the strings that is used to add correct label for each box.\n",
    "PATH_TO_LABELS = 'object_detection/data/mscoco_label_map.pbtxt'\n",
    "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_exist_model(model_name):\n",
    "    model_dir = model_name\n",
    "    model_dir = pathlib.Path(model_dir)/\"saved_model\"\n",
    "\n",
    "    model = tf.saved_model.load(str(model_dir))\n",
    "    model = model.signatures['serving_default']\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_for_single_image(model, image):\n",
    "    image = np.asarray(image)\n",
    "    # The input needs to be a tensor, convert it using `tf.convert_to_tensor`.\n",
    "    input_tensor = tf.convert_to_tensor(image)\n",
    "    # The model expects a batch of images, so add an axis with `tf.newaxis`.\n",
    "    input_tensor = input_tensor[tf.newaxis,...]\n",
    "\n",
    "    # Run inference\n",
    "    output_dict = model(input_tensor)\n",
    "\n",
    "    # All outputs are batches tensors.\n",
    "    # Convert to numpy arrays, and take index [0] to remove the batch dimension.\n",
    "    # We're only interested in the first num_detections.\n",
    "    num_detections = int(output_dict.pop('num_detections'))\n",
    "    output_dict = {key:value[0, :num_detections].numpy() \n",
    "                                 for key,value in output_dict.items()}\n",
    "    output_dict['num_detections'] = num_detections\n",
    "\n",
    "    # detection_classes should be ints.\n",
    "    output_dict['detection_classes'] = output_dict['detection_classes'].astype(np.int64)\n",
    "     \n",
    "    # Handle models with masks:\n",
    "    if 'detection_masks' in output_dict:\n",
    "        # Reframe the the bbox mask to the image size.\n",
    "        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "                            output_dict['detection_masks'], output_dict['detection_boxes'],\n",
    "                             image.shape[0], image.shape[1])            \n",
    "        detection_masks_reframed = tf.cast(detection_masks_reframed > 0.5,\n",
    "                                                                             tf.uint8)\n",
    "        output_dict['detection_masks_reframed'] = detection_masks_reframed.numpy()\n",
    "        \n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_inference_test2(model, image):\n",
    "    # the array based representation of the image will be used later in order to prepare the\n",
    "    # result image with boxes and labels on it.\n",
    "    image_np = np.array(image)\n",
    "    # Actual detection.\n",
    "    output_dict = run_inference_for_single_image(model, image_np)\n",
    "    # Visualization of the results of a detection.\n",
    "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "            image_np,\n",
    "            output_dict['detection_boxes'],\n",
    "            output_dict['detection_classes'],\n",
    "            output_dict['detection_scores'],\n",
    "            category_index,\n",
    "            instance_masks=output_dict.get('detection_masks_reframed', None),\n",
    "            use_normalized_coordinates=True,\n",
    "            min_score_thresh = .7,\n",
    "            line_thickness=8)\n",
    "\n",
    "    return image_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "model_name = \"mask_rcnn_inception_v2_coco_2018_01_28m\"\n",
    "masking_model = load_exist_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read time from 0 ~ to 20 : 5.582s\n",
      "read time from 20 ~ to 40 : 5.929s\n",
      "read time from 40 ~ to 60 : 5.536s\n",
      "read time from 60 ~ to 80 : 5.477s\n",
      "read time from 80 ~ to 100 : 5.944s\n",
      "read time from 100 ~ to 120 : 5.479s\n",
      "read time from 120 ~ to 140 : 5.573s\n",
      "read time from 140 ~ to 160 : 5.673s\n",
      "read time from 160 ~ to 180 : 5.754s\n"
     ]
    }
   ],
   "source": [
    "# 세션 말고 정식 방법을 써보자.\n",
    "# 진짜 훨씬 훨씬 빠르다. 진작 이 방법을 쓸걸 그랬다.\n",
    "\n",
    "# 비디오에서 적용    \n",
    "# 추론 이미지 생성\n",
    "capture = cv2.VideoCapture(\"files/bird.mp4\") # https://www.youtube.com/watch?v=XdlIbNrki5o\n",
    "\n",
    "start = time.time()\n",
    "while True:\n",
    "    ret, frame = capture.read()\n",
    "        \n",
    "    if (capture.get(cv2.CAP_PROP_POS_FRAMES) == capture.get(cv2.CAP_PROP_FRAME_COUNT)):\n",
    "        break\n",
    "        \n",
    "    frame = show_inference_test2(masking_model, frame)\n",
    "\n",
    "    cv2.imshow(\"Object Detections\", frame)\n",
    "    \n",
    "    if capture.get(cv2.CAP_PROP_POS_FRAMES) % 20 == 0:\n",
    "            print(\"read time from {:d} ~ to {:d} : {:.3f}s\".format(\n",
    "                int(capture.get(cv2.CAP_PROP_POS_FRAMES)-20),int(capture.get(cv2.CAP_PROP_POS_FRAMES)),time.time()-start))\n",
    "            start = time.time()\n",
    "    if cv2.waitKey(33) == ord('q'): \n",
    "        cv2.destroyAllWindows()\n",
    "        break\n",
    "    #     cv2.imshow(\"Object Detections\", img)\n",
    "    #     cv2.waitKey(0) \n",
    "    #     cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
